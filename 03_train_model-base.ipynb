{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " model training process:\n",
    "\n",
    "   1. load train image feature set and dev image feature set, this is a two step process:\n",
    "\n",
    "  a. data already split to train,dev, test: load train and dev image id sets\n",
    "\n",
    "   b. load image features for train and dev sets\n",
    "   2. load clean descriptions and prepare add seqstart and seqend to training and dev sequences\n",
    "\n",
    "   3. tokenize train and test sets\n",
    "   4. create the model input sequences, includes a photos input and a tokenized text sequence\n",
    "   5. define the model\n",
    "   6. fit the model and assess loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils.helpers import Config\n",
    "from utils.dataprep import load_set, load_photo_features\n",
    "from utils.dataprep import load_clean_descriptions, get_tokenizer, max_length_desc\n",
    "from utils.inputprep import create_sequences, data_generator\n",
    "\n",
    "c = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ids: 6000, and dev ids: 1000\n",
      "Train photos: 6000, and dev photos: 1000\n"
     ]
    }
   ],
   "source": [
    "# 1. load train and dev images features \n",
    "train = load_set(c.FlickrTextFilePath(\"Flickr_8k.trainImages.txt\"))\n",
    "dev = load_set(c.FlickrTextFilePath(\"Flickr_8k.devImages.txt\"))\n",
    "\n",
    "# use VGG trained features \n",
    "train_features = load_photo_features(c.ExtractedFeaturesFilePath(\"vgg_features.pkl\"), train)\n",
    "dev_features = load_photo_features(c.ExtractedFeaturesFilePath(\"vgg_features.pkl\"), dev)\n",
    "\n",
    "print(\"Train ids: %i, and dev ids: %i\" % (len(train), len(dev)))\n",
    "print(\"Train photos: %i, and dev photos: %i\" % (len(train_features), len(dev_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train descriptions: 6000, and dev descriptions: 1000\n"
     ]
    }
   ],
   "source": [
    "# 2. load clean descriptions for data sets. and load vocabulary \n",
    "\n",
    "train_descriptions = load_clean_descriptions(c.ExtractedFeaturesFilePath('descriptions.txt'), train)\n",
    "dev_descriptions = load_clean_descriptions(c.ExtractedFeaturesFilePath('descriptions.txt'), dev)\n",
    "\n",
    "print(\"Train descriptions: %i, and dev descriptions: %i\" % (len(train_descriptions), len(dev_descriptions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokensizer vocalulary size: 7579, Description max length: 34 \n"
     ]
    }
   ],
   "source": [
    "# 3. tokensize train and dev sets \n",
    "\n",
    "# prepare tokensizer\n",
    "tokenizer = get_tokenizer(c.TokenizerFilePath) \n",
    "max_length = max_length_desc(train_descriptions)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print( \"Tokensizer vocalulary size: %i, Description max length: %i \" % (vocab_size, max_length))\n",
    "# TODO: here we should save the tokenizer for later use, it will be needed when traslating yhat vector to a description \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. create input and validation set sequences\n",
    "   sequences will contain the following components:\n",
    "   - photo features mapped to image ids\n",
    "   - input text sequences mapped to image ids\n",
    "   - output text sequences\n",
    " prepare train and test sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next step takes time and is not needed when using the data_generator approach, commenting it out now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n",
    "#X1test, X2test, ytest = create_sequences(tokenizer, max_length, dev_descriptions, dev_features)\n",
    "# TODO: i am not going to need these when we switch to data generator \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. define and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tmodel.summary()\n",
    "\t# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file=c.ExtractedFeaturesFilePath('model.png'), show_shapes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "dev_data_generator = data_generator(dev_descriptions, dev_features, tokenizer, max_length)\n",
    "# inputs, outputs = next(generator)\n",
    "# print(inputs[0].shape)\n",
    "# print(inputs[1].shape)\n",
    "# print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/ \n",
    "# Why do we need steps_per_epoch ?\n",
    "\n",
    "# Keep in mind that a Keras data generator is meant to loop infinitely — it should never return or exit.\n",
    "\n",
    "# Since the function is intended to loop infinitely, Keras has no ability to determine when one epoch starts \n",
    "# and a new epoch begins.\n",
    "\n",
    "# Therefore, we compute the steps_per_epoch  value as the total number of training data points divided by the batch size. \n",
    "# Once Keras hits this step count it knows that it’s a new epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/\n",
    " Why do we need steps_per_epoch ?\n",
    " Keep in mind that a Keras data generator is meant to loop infinitely — it should never return or exit.\n",
    " Since the function is intended to loop infinitely, Keras has no ability to determine when one epoch starts\n",
    " and a new epoch begins.\n",
    " Therefore, we compute the steps_per_epoch  value as the total number of training data points divided by the batch size.\n",
    " Once Keras hits this step count it knows that it’s a new epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      " - 410s - loss: 4.6529 - val_loss: 4.1411\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.14113, saving model to model-ep001-loss4.675-val_loss4.141.h5\n",
      "Epoch 2/20\n",
      " - 408s - loss: 3.9035 - val_loss: 3.9383\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.14113 to 3.93825, saving model to model-ep002-loss3.926-val_loss3.938.h5\n",
      "Epoch 3/20\n",
      " - 408s - loss: 3.6597 - val_loss: 3.8611\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.93825 to 3.86110, saving model to model-ep003-loss3.682-val_loss3.861.h5\n",
      "Epoch 4/20\n",
      " - 408s - loss: 3.5116 - val_loss: 3.8509\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.86110 to 3.85090, saving model to model-ep004-loss3.535-val_loss3.851.h5\n",
      "Epoch 5/20\n",
      " - 408s - loss: 3.4208 - val_loss: 3.8229\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.85090 to 3.82290, saving model to model-ep005-loss3.444-val_loss3.823.h5\n",
      "Epoch 6/20\n",
      " - 408s - loss: 3.3536 - val_loss: 3.8461\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.82290\n",
      "Epoch 7/20\n",
      " - 408s - loss: 3.3036 - val_loss: 3.8587\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.82290\n",
      "Epoch 8/20\n",
      " - 408s - loss: 3.2684 - val_loss: 3.9022\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.82290\n",
      "Epoch 9/20\n"
     ]
    }
   ],
   "source": [
    " history = model.fit_generator(\n",
    " \ttrain_data_generator,\n",
    " \tepochs=20,\n",
    " \tsteps_per_epoch=len(train_descriptions),\n",
    " \tverbose=2, # 1: progress, 2: one line per epoch\n",
    " \tvalidation_data= dev_data_generator,\n",
    " \tvalidation_steps=len(dev_descriptions),\n",
    " \tcallbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save history\n",
    "import pickle\n",
    "\n",
    "with open(c.ExtractedFeaturesFilePath(\"model_run_history.pkl\"), \"wb\") as pcklfile:\n",
    "\tpickle.dump(history, pcklfile)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
