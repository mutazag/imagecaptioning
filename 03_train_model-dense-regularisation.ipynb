{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set inputs to train based for different CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_file_name = \"densenet_features.pkl\"\n",
    "checkpoint_output_file = 'dense_regu-model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "history_file_name = \"dense_regu-model_run_history.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# you dont need to touch any of the below\n",
    "\n",
    "just run and wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.helpers import Config\n",
    "from utils.dataprep import load_set, load_photo_features\n",
    "from utils.dataprep import load_clean_descriptions, get_tokenizer, max_length_desc\n",
    "from utils.inputprep import create_sequences, data_generator\n",
    "\n",
    "c = Config()\n",
    "\n",
    "\n",
    "feature_file_name = c.ExtractedFeaturesFilePath(feature_file_name)\n",
    "checkpoint_output_file = checkpoint_output_file\n",
    "history_file_name = c.ExtractedFeaturesFilePath(history_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ids: 6000, and dev ids: 1000\n",
      "Train photos: 6000, and dev photos: 1000\n"
     ]
    }
   ],
   "source": [
    "# 1. load train and dev images features \n",
    "train = load_set(c.FlickrTextFilePath(\"Flickr_8k.trainImages.txt\"))\n",
    "dev = load_set(c.FlickrTextFilePath(\"Flickr_8k.devImages.txt\"))\n",
    "\n",
    "# use VGG trained features \n",
    "train_features = load_photo_features(feature_file_name, train)\n",
    "dev_features = load_photo_features(feature_file_name, dev)\n",
    "\n",
    "print(\"Train ids: %i, and dev ids: %i\" % (len(train), len(dev)))\n",
    "print(\"Train photos: %i, and dev photos: %i\" % (len(train_features), len(dev_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[\"3009047603_28612247d2\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train descriptions: 6000, and dev descriptions: 1000\n"
     ]
    }
   ],
   "source": [
    "# 2. load clean descriptions for data sets. and load vocabulary \n",
    "\n",
    "train_descriptions = load_clean_descriptions(c.ExtractedFeaturesFilePath('descriptions.txt'), train)\n",
    "dev_descriptions = load_clean_descriptions(c.ExtractedFeaturesFilePath('descriptions.txt'), dev)\n",
    "\n",
    "print(\"Train descriptions: %i, and dev descriptions: %i\" % (len(train_descriptions), len(dev_descriptions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokensizer vocalulary size: 7579, Description max length: 34 \n"
     ]
    }
   ],
   "source": [
    "# 3. tokensize train and dev sets \n",
    "\n",
    "# prepare tokensizer\n",
    "tokenizer = get_tokenizer(c.TokenizerFilePath) \n",
    "max_length = max_length_desc(train_descriptions)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print( \"Tokensizer vocalulary size: %i, Description max length: %i \" % (vocab_size, max_length))\n",
    "# TODO: here we should save the tokenizer for later use, it will be needed when traslating yhat vector to a description \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 4. define and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length, input_dim = 4096):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(input_dim,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256,  kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tmodel.summary()\n",
    "\t# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          262400      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,741,531\n",
      "Trainable params: 4,741,531\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = train_features[\"3009047603_28612247d2\"].shape[1]\n",
    "model = define_model(vocab_size, max_length, input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "dev_data_generator = data_generator(dev_descriptions, dev_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "filepath = checkpoint_output_file # 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    " history = model.fit_generator(\n",
    " \ttrain_data_generator,\n",
    " \tepochs=20,\n",
    " \tsteps_per_epoch=len(train_descriptions),\n",
    " \tverbose=2, # 1: progress, 2: one line per epoch\n",
    " \tvalidation_data= dev_data_generator,\n",
    " \tvalidation_steps=len(dev_descriptions),\n",
    " \tcallbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save history\n",
    "import pickle\n",
    "\n",
    "with open(history_file_name, \"wb\") as pcklfile:\n",
    "    pickle.dump(history, pcklfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
