{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from numpy import array\n",
    "\n",
    "from os import listdir\n",
    "from pickle import dump, load\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import string\n",
    "\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "###GloVe\n",
    "from numpy import asarray\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the file containing all of the descriptions\n",
    "\n",
    "def load_doc(filename):\n",
    "    #open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    #read all text\n",
    "    text = file.read()\n",
    "    #close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "filename = '../input/flickr8k/flickr8k_text/Flickr8k.token.txt'\n",
    "#load descriptions \n",
    "doc = load_doc(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    #process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        #slit line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        #take the first token as the image id, the rest as a description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        #remove filename extension from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        #convert descrption tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        #create the list if needed\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        #store description\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping\n",
    "\n",
    "#parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's clean the text. Perforrming the necessary tasks of text mining:\n",
    "- converting words to lowercase\n",
    "- removing all the punctuations\n",
    "- removing all the words that are 1 character or less in length\n",
    "- remove all words with numbers in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    #prepare translation table for removing the punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            #tokenize\n",
    "            desc = desc.split()\n",
    "            #convert to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            #remove punctuation from each token\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            #remove 1 character words \n",
    "            desc = [word for word in desc if len(word) > 1]\n",
    "            #remove tokens with numbers in them\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            #store as a string\n",
    "            desc_list[i] = ' '.join(desc)\n",
    "        \n",
    "#clean descriptions\n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we summarize the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "    #build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "#summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' %len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "#save descriptions to the file\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load doc into memory\n",
    "#use the function load_doc defines above\n",
    "\n",
    "#load a pre-defines list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        #skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        #get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function __load_clean_descriptions()__ defined below loads the cleaned text descriptions from ‘descriptions.txt‘ for a given set of identifiers and returns a dictionary of identifiers to lists of text descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using  strings __startseq__ and __endseq__ for first-word and last word signal purpose. These tokens are added to the loaded descriptions as they are loaded. It is important to do this now before we encode the text so that the tokens are also encoded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    #load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        #split line by whitespace\n",
    "        tokens = line.split()\n",
    "        #split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        #skip images not in set\n",
    "        if image_id in dataset:\n",
    "            #create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            #wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            #store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    #load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    #filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training dataset (6K)\n",
    "filename = '../input/flickr8k/flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' %len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train = %d' %len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('../input/01-image-features/densenet_features.pkl', train)\n",
    "print('Photos: train = %d' %len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation dataset\n",
    "filename = '../input/flickr8k/flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dev = load_set(filename)\n",
    "print('Dataset: %d' % len(dev))\n",
    "# descriptions\n",
    "dev_descriptions = load_clean_descriptions('descriptions.txt', dev)\n",
    "print('Descriptions: dev=%d' % len(dev_descriptions))\n",
    "# photo features\n",
    "dev_features = load_photo_features('../input/01-image-features/densenet_features.pkl', dev)\n",
    "print('Photos: train=%d' % len(dev_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description text will need to be encoded to numbers before it can be presented to the model as in input or compared to the model’s predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert a dictionary of clean descriptions to a list of descreptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "#fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "#prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' %vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each image identifier\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # walk through each description for the image\n",
    "        for desc in desc_list:\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            # split one sequence into multiple X,y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                # store\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_GloVe = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GloVe\n",
    "#bring in GloVe word embedding model to map our vocab into vectors\n",
    "\n",
    "if use_GloVe:\n",
    "    # Load GloVe vectors\n",
    "    embeddings_index = {} # empty dictionary\n",
    "    f = open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt', encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GloVe \n",
    "# create embedding matrix to inject into model\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "if use_GloVe:\n",
    "    embedding_dim = 200\n",
    "    # Get dense vector for each of word in vocabulary\n",
    "    embedding_matrix = zeros((vocab_size, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        #if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in the embedding index will be all zeros\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(1024,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2) \n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    \n",
    "    if use_GloVe:\n",
    "        ###GloVe\n",
    "        # now set the embedding weights\n",
    "        model.layers[2].set_weights([embedding_matrix])\n",
    "        model.layers[2].trainable = False\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    # plot_model(model, to_file='model.png', show_shapes = True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FITTING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train With Progressive Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length):\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
    "            yield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we are calling the create_sequence() function to create a batch worth of data for a single photo rather than an entire dataset. This means that we must update the create_sequences() function to delete the “iterate over all descriptions” for-loop.\n",
    "\n",
    "The updated function is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model, run epochs manually and save after each epoch\n",
    "\n",
    "label = 'model'\n",
    "epoch_loop = 2\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "#checkpoint = ModelCheckpoint(model_filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "\n",
    "history = {}\n",
    "min_val_loss_epoch = -1\n",
    "min_val_loss = 100\n",
    "\n",
    "for i in range(epoch_loop):\n",
    "    # fit for one epoch\n",
    "    train_generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    validation_generator = data_generator(dev_descriptions, dev_features, tokenizer, max_length)\n",
    "    hist = model.fit_generator(train_generator,\n",
    "                        epochs=1,\n",
    "                        steps_per_epoch=len(train_descriptions),\n",
    "                        verbose=1,\n",
    "                        #callbacks = [checkpoint],\n",
    "                        validation_data = validation_generator,\n",
    "                        validation_steps = len(dev_descriptions))\n",
    "    # save model\n",
    "    model_filepath = label + '_epoch_' + str(i) + '.h5'\n",
    "    model.save(model_filepath)\n",
    "    history[i] = hist.history\n",
    "    if history[i][\"val_loss\"][0] <= min_val_loss:\n",
    "        min_val_loss_epoch = i\n",
    "        min_val_loss = hist.history[\"val_loss\"][0]\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "# save history \n",
    "history_filepath = label + '_history_' + str(end) + '.pkl'\n",
    "with open(history_filepath, \"wb\") as pcklfile: \n",
    "    dump(history, pcklfile)\n",
    "\n",
    "print(\"Total time taken: {}\".format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define evaluation\n",
    "\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test set\n",
    " \n",
    "# load test set\n",
    "filename = '../input/flickr8k/flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('../input/01-image-features/densenet_features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the best epoch result\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# identify the best one\n",
    "best = min_val_loss_epoch\n",
    "\n",
    "# load the model\n",
    "best_filepath = label + '_epoch_' + str(best) + '.h5'\n",
    "model = load_model(best_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate on test set\n",
    "\n",
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# evaluate model\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBC\n",
    "# get features for everything so that we can apply prediction to more than just the test set\n",
    "# (i'm looking at you, curly!)\n",
    "\n",
    "def demo_captions(model, photos, tokenizer, max_length, demo_list):\n",
    "    captions = list()\n",
    "    for key in demo_list:\n",
    "        # generate description\n",
    "        caption = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        captions.append(caption)\n",
    "    return(captions)\n",
    "\n",
    "curly = '1015118661_980735411b'\n",
    "\n",
    "demo_list = ('3497224764_6e17544e0d','3044500219_778f9f2b71','3119076670_64b5340530','1220401002_3f44b1f3f7', '241345844_69e1c22464', '2762301555_48a0d0aa24', '3364861247_d590fa170d', '3406930103_4db7b4dde0', '1343426964_cde3fb54e8', '2984174290_a915748d77', '2913965136_2d00136697', '2862004252_53894bb28b', '3697359692_8a5cdbe4fe')\n",
    "demo_path = '../input/flickr8k/flickr8k_dataset/Flicker8k_Dataset/'\n",
    "demo_captions_list = demo_captions(model, test_features, tokenizer, max_length, demo_list)\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "for k in range(len(demo_list)):\n",
    "    img=mpimg.imread(demo_path + demo_list[k]+'.jpg')\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(demo_captions_list[k])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
